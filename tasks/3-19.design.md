# Design 3-19 - 修复流式响应过度拆分问题

## Requirements

### 问题描述
当前 LangGraph 的 messages 模式流式响应存在严重问题：
1. **过度碎片化**：每个中文字符作为单独的 chunk 发送
2. **前端显示错误**：使用替换而非追加，只显示最后一个字符
3. **数据库爆炸**：每个字符都创建一个新的数据库记录
4. **用户体验极差**：无法看到完整的响应内容

### 测试结果
- 平均 chunk 长度：1-2 个字符
- 一句话被拆分成 20-50 个 chunks
- 数据库写入次数等于字符数

### 需求目标
1. **合理的 chunk 大小**：按词、短语或句子发送，而非单个字符
2. **正确的前端累积**：追加内容而非替换
3. **优化数据库写入**：减少写入次数，提高性能
4. **流畅的用户体验**：实时看到响应内容逐步显示

### 技术约束
1. 保持与 LangGraph messages 模式的兼容性
2. 不影响工具调用事件的处理
3. 保持流式响应的实时性
4. 向后兼容现有的前端代码

## Solution

### 研究发现
基于 LangGraph 官方文档和测试：
- LangGraph 的 messages 模式确实按 token 级别流式输出
- 对于中文，通常是 1-2 个字符一个 chunk
- 这是 LLM 的正常行为，需要在应用层进行优化

### 详细实现方案

#### 1. 后端 chunk 累积机制（任务 3-19-2）

在 `backend/app/agents/conversation_handler.py` 的 `stream_response` 方法中实现智能 chunk 累积：

```python
class ChunkAccumulator:
    def __init__(self, 
                 min_chunk_size: int = 10,  # 最小chunk大小（字符数）
                 max_wait_time: float = 0.5,  # 最大等待时间（秒）
                 delimiter_pattern: str = r'[。！？；\n]'):  # 分隔符模式
        self.buffer = ""
        self.last_emit_time = time.time()
        self.min_chunk_size = min_chunk_size
        self.max_wait_time = max_wait_time
        self.delimiter_pattern = delimiter_pattern
    
    def add(self, content: str) -> Optional[str]:
        """添加内容到缓冲区，返回应该发送的内容（如果有）"""
        self.buffer += content
        
        # 检查是否应该发送
        if self.should_emit():
            return self.flush()
        return None
    
    def should_emit(self) -> bool:
        """判断是否应该发送缓冲内容"""
        # 1. 达到分隔符（句子结束）
        if re.search(self.delimiter_pattern, self.buffer):
            return True
        
        # 2. 缓冲区达到最小大小
        if len(self.buffer) >= self.min_chunk_size:
            return True
        
        # 3. 超过最大等待时间
        if time.time() - self.last_emit_time > self.max_wait_time:
            return True
        
        return False
    
    def flush(self) -> str:
        """清空并返回缓冲区内容"""
        content = self.buffer
        self.buffer = ""
        self.last_emit_time = time.time()
        return content
```

修改 `stream_response` 方法：
```python
async def stream_response(self, state: AgentState, conversation_id: str):
    accumulator = ChunkAccumulator()
    response_id = str(uuid.uuid4())
    accumulated_content = ""  # 用于数据库写入
    
    try:
        async for chunk in self.graph.astream_events(
            state, 
            version="v2",
            stream_mode="messages"
        ):
            # 处理工具调用事件（保持不变）
            if chunk["event"] == "on_tool_start":
                yield self._handle_tool_start(chunk)
            elif chunk["event"] == "on_tool_end":
                yield self._handle_tool_end(chunk)
            
            # 处理消息内容
            elif chunk["event"] == "on_chat_model_stream":
                chunk_obj = chunk.get("data", {}).get("chunk")
                if hasattr(chunk_obj, 'content') and chunk_obj.content:
                    # 累积内容
                    emit_content = accumulator.add(chunk_obj.content)
                    accumulated_content += chunk_obj.content
                    
                    if emit_content:
                        yield {
                            "type": "agent_response_chunk",
                            "content": emit_content,
                            "timestamp": datetime.now(timezone.utc).isoformat(),
                            "id": response_id
                        }
        
        # 发送剩余内容
        final_content = accumulator.flush()
        if final_content:
            yield {
                "type": "agent_response_chunk",
                "content": final_content,
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "id": response_id
            }
        
        # 一次性写入完整消息到数据库
        if accumulated_content:
            ai_msg = ConversationMessage(
                id=response_id,
                conversation_id=conversation_id,
                role="assistant",
                content=accumulated_content,
                timestamp=datetime.now(timezone.utc)
            )
            self.db.add(ai_msg)
            self.db.commit()
        
        # 发送完成信号
        yield {
            "type": "conversation_complete",
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Stream response error: {e}")
        yield {
            "type": "error",
            "message": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
```

#### 2. 前端消息累积修复（任务 3-19-3）

修改 `frontend/src/stores/chatStore.ts` 的消息处理逻辑：

```typescript
// Agent 响应片段（流式）- 修复版
socket.on('agent_response_chunk', (data) => {
  console.log('收到 Agent 响应片段:', data);
  const { streamingMessageId, updateMessage, addMessage, messages } = get();
  
  if (streamingMessageId && data.id === streamingMessageId) {
    // 更新现有的流式消息 - 追加而非替换
    const currentMessage = messages.find(m => m.id === streamingMessageId);
    if (currentMessage) {
      updateMessage(streamingMessageId, {
        content: currentMessage.content + data.content,  // 追加内容
        timestamp: new Date(data.timestamp)
      });
    }
  } else {
    // 创建新的流式消息
    const newMessage: ChatMessage = {
      id: data.id || uuidv4(),
      type: 'agent',
      content: data.content,
      timestamp: new Date(data.timestamp),
      isStreaming: true
    };
    addMessage(newMessage);
    set({ streamingMessageId: newMessage.id });
  }
});
```

#### 3. 数据库写入优化（任务 3-19-4）

- 移除 stream_response 中的即时数据库写入
- 只在消息完成后一次性写入
- 使用事务确保数据一致性
- 考虑使用批量写入进一步优化

#### 4. 配置和调优

添加环境变量支持动态配置：
```python
# backend/app/core/config.py
class Settings:
    # Streaming 配置
    CHUNK_MIN_SIZE: int = 10  # 最小chunk大小
    CHUNK_MAX_WAIT: float = 0.5  # 最大等待时间（秒）
    CHUNK_DELIMITER_PATTERN: str = r'[。！？；\n]'  # 分隔符模式
```

## Tests

### 单元测试
1. chunk 累积器的各种边界情况
2. 不同语言（中文、英文）的处理
3. 标点符号和特殊字符的处理

### 集成测试
1. 完整对话流程的响应显示
2. 工具调用不受影响
3. 长文本响应的流畅性

### 性能测试
1. 数据库写入次数减少 90% 以上
2. 响应延迟保持在可接受范围
3. 内存使用合理