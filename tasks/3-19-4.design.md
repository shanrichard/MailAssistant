# Design 3-19-4 - 优化数据库写入策略（批量写入或完成后写入）

## Requirements

优化消息数据库写入策略，解决当前每个 chunk 都写入数据库的性能问题：
1. 减少数据库写入次数（从每字符一次到每消息一次）
2. 保证数据完整性和一致性
3. 考虑异常情况下的数据恢复
4. 优化事务处理和批量操作

## Solution

### 策略选择：完成后写入

经过权衡，选择"完成后写入"策略：
- **优点**：实现简单、性能最优、数据一致性好
- **缺点**：流式过程中断可能丢失部分消息
- **缓解措施**：添加内存缓存和恢复机制

### 实现方案

#### 1. 移除即时写入逻辑

修改 `backend/app/agents/conversation_handler.py`：

```python
async def stream_response(self, state: AgentState, conversation_id: str):
    """流式处理响应 - 优化数据库写入"""
    accumulator = ChunkAccumulator(
        min_chunk_size=self.settings.CHUNK_MIN_SIZE,
        max_wait_time=self.settings.CHUNK_MAX_WAIT,
        delimiter_pattern=self.settings.CHUNK_DELIMITER_PATTERN
    )
    response_id = str(uuid.uuid4())
    accumulated_content = ""
    
    try:
        async for chunk in self.graph.astream_events(
            state, 
            version="v2",
            stream_mode="messages"
        ):
            # 处理各种事件...
            elif chunk["event"] == "on_chat_model_stream":
                chunk_obj = chunk.get("data", {}).get("chunk")
                if hasattr(chunk_obj, 'content') and chunk_obj.content:
                    # 只累积内容，不写数据库
                    emit_content = accumulator.add(chunk_obj.content)
                    accumulated_content += chunk_obj.content
                    
                    if emit_content:
                        yield {
                            "type": "agent_response_chunk",
                            "content": emit_content,
                            "timestamp": datetime.now(timezone.utc).isoformat(),
                            "id": response_id
                        }
        
        # 发送剩余内容
        final_content = accumulator.flush()
        if final_content:
            yield {
                "type": "agent_response_chunk",
                "content": final_content,
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "id": response_id
            }
        
        # 流式完成后，一次性写入数据库
        if accumulated_content:
            await self._save_complete_message(
                response_id=response_id,
                conversation_id=conversation_id,
                content=accumulated_content,
                metadata={
                    "stream_completed": True,
                    "chunk_count": accumulator.total_chunks_emitted
                }
            )
        
        yield {
            "type": "conversation_complete",
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Stream response error: {e}")
        # 错误情况下也尝试保存已累积的内容
        if accumulated_content:
            await self._save_incomplete_message(
                response_id=response_id,
                conversation_id=conversation_id,
                content=accumulated_content,
                error=str(e)
            )
        
        yield {
            "type": "error",
            "message": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
```

#### 2. 优化数据库写入方法

添加专门的数据库写入方法：

```python
async def _save_complete_message(
    self, 
    response_id: str,
    conversation_id: str,
    content: str,
    metadata: dict = None
):
    """保存完整的消息到数据库"""
    try:
        message = ConversationMessage(
            id=response_id,
            conversation_id=conversation_id,
            role="assistant",
            content=content,
            timestamp=datetime.now(timezone.utc),
            metadata=metadata or {}
        )
        
        # 使用事务确保原子性
        async with self.db.begin():
            self.db.add(message)
            await self.db.commit()
            
        logger.info(f"Saved message {response_id} with {len(content)} chars")
        
    except Exception as e:
        logger.error(f"Failed to save message: {e}")
        await self.db.rollback()
        raise

async def _save_incomplete_message(
    self,
    response_id: str,
    conversation_id: str,
    content: str,
    error: str
):
    """保存不完整的消息（错误情况）"""
    try:
        message = ConversationMessage(
            id=response_id,
            conversation_id=conversation_id,
            role="assistant",
            content=content,
            timestamp=datetime.now(timezone.utc),
            metadata={
                "incomplete": True,
                "error": error,
                "content_length": len(content)
            }
        )
        
        async with self.db.begin():
            self.db.add(message)
            await self.db.commit()
            
    except Exception as e:
        logger.error(f"Failed to save incomplete message: {e}")
```

#### 3. 添加缓存层（可选优化）

为了进一步优化性能，可以添加 Redis 缓存：

```python
from app.core.redis import redis_client

class MessageCache:
    """消息缓存管理器"""
    
    @staticmethod
    async def cache_streaming_content(
        response_id: str, 
        content: str,
        ttl: int = 300  # 5分钟过期
    ):
        """缓存流式内容"""
        key = f"streaming:{response_id}"
        await redis_client.setex(key, ttl, content)
    
    @staticmethod
    async def get_cached_content(response_id: str) -> Optional[str]:
        """获取缓存的内容"""
        key = f"streaming:{response_id}"
        return await redis_client.get(key)
    
    @staticmethod
    async def append_cached_content(
        response_id: str,
        content: str,
        ttl: int = 300
    ):
        """追加内容到缓存"""
        key = f"streaming:{response_id}"
        existing = await redis_client.get(key) or ""
        await redis_client.setex(key, ttl, existing + content)
```

#### 4. 批量写入优化（进阶）

对于高并发场景，实现批量写入：

```python
from asyncio import Queue, create_task
from typing import List

class BatchMessageWriter:
    """批量消息写入器"""
    
    def __init__(self, batch_size: int = 10, flush_interval: float = 1.0):
        self.batch_size = batch_size
        self.flush_interval = flush_interval
        self.queue: Queue[ConversationMessage] = Queue()
        self.worker_task = None
    
    async def start(self):
        """启动批量写入工作器"""
        self.worker_task = create_task(self._worker())
    
    async def add_message(self, message: ConversationMessage):
        """添加消息到批量队列"""
        await self.queue.put(message)
    
    async def _worker(self):
        """批量写入工作器"""
        batch: List[ConversationMessage] = []
        
        while True:
            try:
                # 收集批量消息
                timeout = self.flush_interval if batch else None
                message = await asyncio.wait_for(
                    self.queue.get(), 
                    timeout=timeout
                )
                batch.append(message)
                
                # 达到批量大小，立即写入
                if len(batch) >= self.batch_size:
                    await self._flush_batch(batch)
                    batch = []
                    
            except asyncio.TimeoutError:
                # 超时，写入当前批次
                if batch:
                    await self._flush_batch(batch)
                    batch = []
    
    async def _flush_batch(self, batch: List[ConversationMessage]):
        """写入一批消息"""
        try:
            async with self.db.begin():
                self.db.add_all(batch)
                await self.db.commit()
            logger.info(f"Batch wrote {len(batch)} messages")
        except Exception as e:
            logger.error(f"Batch write failed: {e}")
            await self.db.rollback()
```

## Tests

### 性能测试

创建 `backend/tests/test_db_optimization.py`：

```python
import pytest
import asyncio
from datetime import datetime
from app.models import ConversationMessage
from app.agents.conversation_handler import ConversationHandler

@pytest.mark.asyncio
async def test_single_write_performance(db_session, benchmark):
    """测试单次写入性能"""
    handler = ConversationHandler(db_session)
    
    # 模拟完整消息
    content = "这是一个测试消息" * 100  # 1000+ 字符
    
    async def write_message():
        await handler._save_complete_message(
            response_id="test-id",
            conversation_id="conv-id",
            content=content
        )
    
    # 基准测试
    result = await benchmark(write_message)
    assert result is None

@pytest.mark.asyncio
async def test_no_intermediate_writes(db_session, mocker):
    """验证流式过程中没有中间写入"""
    handler = ConversationHandler(db_session)
    
    # Mock 数据库写入
    mock_add = mocker.patch.object(db_session, 'add')
    
    # 模拟流式响应
    chunks = ["你", "好", "，", "世", "界", "。"]
    response_id = "test-response"
    
    # 处理所有 chunks
    for chunk in chunks:
        # 这里应该只是累积，不写数据库
        pass
    
    # 验证没有中间写入
    assert mock_add.call_count == 0
    
    # 完成后应该有一次写入
    await handler._save_complete_message(
        response_id=response_id,
        conversation_id="test-conv",
        content="".join(chunks)
    )
    
    assert mock_add.call_count == 1
```

### 数据完整性测试

```python
@pytest.mark.asyncio
async def test_error_recovery(db_session):
    """测试错误情况下的数据恢复"""
    handler = ConversationHandler(db_session)
    
    # 模拟流式中断
    accumulated = "部分完成的消息内容"
    error = "Connection lost"
    
    await handler._save_incomplete_message(
        response_id="error-msg",
        conversation_id="test-conv",
        content=accumulated,
        error=error
    )
    
    # 验证消息被保存且标记为不完整
    msg = db_session.query(ConversationMessage).filter_by(
        id="error-msg"
    ).first()
    
    assert msg is not None
    assert msg.content == accumulated
    assert msg.metadata["incomplete"] is True
    assert msg.metadata["error"] == error
```

### 负载测试

使用 locust 进行负载测试，对比优化前后的性能：

```python
# backend/tests/locustfile.py
from locust import HttpUser, task, between

class ChatUser(HttpUser):
    wait_time = between(1, 3)
    
    @task
    def send_message(self):
        self.client.post("/api/chat/message", json={
            "content": "测试消息",
            "conversation_id": "test-conv"
        })
```

预期性能提升：
- 数据库写入次数减少 95%+
- 响应时间降低 30%+
- 数据库负载降低 80%+